"""
MultiServerMCP í´ë¼ì´ì–¸íŠ¸ - ì—¬ëŸ¬ MCP ì„œë²„ì™€ì˜ ì—°ê²°ì„ ê´€ë¦¬
"""

import os
import json
import asyncio
from typing import Any, Dict, List

import aiohttp                 
from dotenv import load_dotenv 
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools  

from utility.utils import to_ollama_tool_description, strip_code_block


load_dotenv()

# Ollama ì„œë²„ ê´€ë ¨ ì„¤ì •
OLLAMA_URL = os.getenv("OLLAMA_URL") 
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL")

# MCP ì„œë²„ ì„¤ì •
MCP_SERVERS_CONFIG = {
    "oracle-db": {
        "command": "python3",
        "args": ["mcp_server/server_oracle-db.py"],
        "transport": "stdio"
    },
    "memory": {
        "command": "python3",
        "args": ["mcp_server/server_memory.py"],
        "transport": "stdio"
    }
}


# MultiServerMCPClient ê°ì²´ ìƒì„±
mcp_client = MultiServerMCPClient(MCP_SERVERS_CONFIG)


# ì„¸ì…˜ ë° ë„êµ¬ ê´€ë¦¬ ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” 
server_sessions = {}  # ì„œë²„ë³„ ì„¸ì…˜ ì €ì¥
all_tools = []        # ëª¨ë“  ì„œë²„ì˜ ë„êµ¬ ì €ì¥
current_session_id = "default"  # ëŒ€í™” ì„¸ì…˜ ID: í˜„ì¬ êµ¬í˜„ì—ì„œëŠ” ë‹¨ìˆœí•˜ê²Œ ëª¨ë“  ëŒ€í™”ê°€ "default" ì„¸ì…˜ì— ì €ì¥ë˜ë¯€ë¡œ, í•œ ëª…ì˜ ì‚¬ìš©ìê°€ í•˜ë‚˜ì˜ ì—°ì†ëœ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ëŠ” ìƒí™©ì— ì í•©


# ì„¸ì…˜ ê´€ë ¨ í•¨ìˆ˜ ì •ì˜
async def initialize_mcp_sessions():
    """
    ëª¨ë“  MCP ì„œë²„ì— ëŒ€í•œ ì„¸ì…˜ì„ ì´ˆê¸°í™”í•˜ê³ , ê° ì„œë²„ë³„ ë„êµ¬ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    global server_sessions, all_tools
    
    for server_name in MCP_SERVERS_CONFIG.keys():
        try:
            # MCP ì„œë²„ ì„¸ì…˜ ì´ˆê¸°í™”
            session = mcp_client.session(server_name) 
            server_sessions[server_name] = session
            
            # ì„œë²„ ë„êµ¬ ë¡œë“œ
            session_context = await session.__aenter__()  # ì„œë²„ START
            tools = await load_mcp_tools(session_context) # [StructuredTool(name='')...] -> LangGraphì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹
            all_tools.extend(tools)
            
            print(f"âœ… {server_name} ì„œë²„ ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ ({len(tools)}ê°œ ë„êµ¬)\n")
            
        except Exception as e:
            print(f"âŒ {server_name} ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}\n")

async def cleanup_mcp_sessions():
    """
    ëª¨ë“  MCP ì„œë²„ ì„¸ì…˜ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
    """
    global server_sessions
    
    for server_name, session in server_sessions.items():
        try:
            await session.__aexit__(None, None, None) # ì„œë²„ EXIT 
            print(f"ğŸ”Œ {server_name} ì„¸ì…˜ ì¢…ë£Œë¨")
        except Exception as e:
            print(f"âš ï¸ {server_name} ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    server_sessions.clear()
    all_tools.clear()


# ëŒ€í™” ì´ë ¥ ì €ì¥ ê´€ë ¨ í•¨ìˆ˜ ì •ì˜
async def save_message_to_memory(role: str, content: str, session_id: str = None):
    """
    í•˜ë‚˜ì˜ ë©”ì„¸ì§€ë¥¼ ë©”ëª¨ë¦¬ ì„œë²„ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    if session_id is None:
        session_id = current_session_id
    
    # "save_message" ë„êµ¬ ì¶”ì¶œ 
    save_msg_tool = None
    for tool in all_tools:
        if tool.name == "save_message":
            save_msg_tool = tool # StructuredTool(name='')
            break
    
    # "save_message" ë„êµ¬ ì‹¤í–‰
    if save_msg_tool:
        try:
            result = await save_msg_tool.ainvoke({ # LangChain ë˜ëŠ” LangGraph ì—ì´ì „íŠ¸ê°€ MCP ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê³  ì‹¤í–‰í•˜ëŠ” ê³¼ì •ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ê³ ìˆ˜ì¤€ ë˜í¼ í•¨ìˆ˜
                "session_id": session_id,
                "role": role,
                "content": content
            })
            print(f"ğŸ’¾ {role} ë©”ì‹œì§€ ì €ì¥ë¨:\n{result}")
        except Exception as e:
            print(f"âš ï¸ ë©”ì‹œì§€ ì €ì¥ ì‹¤íŒ¨: {str(e)}")


# í•µì‹¬ ë¡œì§ ê´€ë ¨ í•¨ìˆ˜ ì •ì˜
async def query_llm(question: str, tool_descriptions: List[str]) -> str:
    """
    LLM í˜¸ì¶œì„ í†µí•´ ì‚¬ìš©í•  ì ì ˆí•œ toolë“¤ì„ ì„ íƒí•˜ê³ , ì´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    async with aiohttp.ClientSession() as session:
        # [1ì°¨ ìš”ì²­] MCP ë„êµ¬ë“¤ ì¤‘ ì ì ˆí•œ ê²ƒì„ ì„ íƒí•˜ì—¬ json í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        # TODO í…ŒìŠ¤íŠ¸ ë° ìˆ˜ì • í•„ìš” 
        system_prompt = (
            "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ MCP ë„êµ¬ë¥¼ í™œìš©í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n"
            "ì•„ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì…ë‹ˆë‹¤:\n\n"
            + "\n\n".join(tool_descriptions) +
            "\n\ní•„ìš”í•œ ë„êµ¬ë§Œ ì„ íƒí•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:\n"
            '''[
  {"function_name": "get_schema_info", "arguments": {}},
  {"function_name": "generate_sql", "arguments": {"natural_query": "...", "schema_info": "..." }},
  {"function_name": "validate_sql", "arguments": {"sql": "..." }},
  {"function_name": "execute_sql", "arguments": {"exec_sql": "..." }}
]'''
            "\n\në©”ëª¨ë¦¬ ê´€ë ¨ ë„êµ¬ ì‚¬ìš©ë²•:\n"
            "- ì´ì „ ëŒ€í™”ë¥¼ ì¡°íšŒí•˜ë ¤ë©´: get_messages\n"
            "- íŠ¹ì • í‚¤ì›Œë“œë¡œ ëŒ€í™”ë¥¼ ê²€ìƒ‰í•˜ë ¤ë©´: search_messages + query\n"
            "- ì„¸ì…˜ ëª©ë¡ì„ ë³´ë ¤ë©´: list_sessions\n"
            "- session_idëŠ” ìë™ìœ¼ë¡œ ì¶”ê°€ë˜ë¯€ë¡œ ìƒëµ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\n"
            "ë§Œì¼, ì„ íƒëœ ë„êµ¬ê°€ ì—†ë‹¤ë©´ ì‚¬ì „ì— í•™ìŠµí•œ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•˜ì„¸ìš”:\n"
        )
        payload = {
            "model": OLLAMA_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            "stream": False
        }
        async with session.post(f"{OLLAMA_URL}/v1/chat/completions", json=payload) as resp:
            data = await resp.json()
            first_reply = data["choices"][0]["message"]["content"]
            cleaned_first_reply = strip_code_block(first_reply)
            
        # ë„êµ¬ í˜¸ì¶œ íŒŒì•½
        try: 
            print(f'\n[DEBUG] LLM ë„êµ¬ í˜¸ì¶œ ê³„íš:\n{cleaned_first_reply}')
            parsed_calls = json.loads(cleaned_first_reply)
        except Exception as e:
            print("\n[DEBUG] âŒ ì„ íƒëœ ë„êµ¬ ì—†ìŒ â†’ ë‹µë³€ ê·¸ëŒ€ë¡œ ì¶œë ¥\n")
            return cleaned_first_reply.strip()

        # ì„ íƒëœ ë„êµ¬ íŒŒì‹± í›„ ìˆœì°¨ ì‹¤í–‰
        executed_results = []
        memory = {}  # ì´ì „ ê²°ê³¼ ì €ì¥ìš©
        for call in parsed_calls:
            fn_name = call.get("function_name") or parsed_calls.get("function") or parsed_calls.get("name")
            args = call.get("arguments", {})

            if not fn_name:
                continue

            # ë©”ëª¨ë¦¬ ê´€ë ¨ ë„êµ¬ë“¤ì— session_id ìë™ ì¶”ê°€
            if fn_name in ["get_messages", "search_messages", "save_message"]:
                if "session_id" not in args:
                    args["session_id"] = current_session_id

            if fn_name == "generate_sql":
                args["natural_query"] =  question
                args["schema_info"] = memory.get("get_schema_info", {})
                # print('[DEBUG] generate_sql args: ', args)

            if fn_name == "validate_sql":
                args["sql"] = memory.get("generate_sql", "")
                # print('[DEBUG] validate_sql args: ', args)

            if fn_name == "execute_sql":
                validation_res = json.loads(memory.get("validate_sql", "")) # langchain_mcp_adaptersê°€ validate_sql()ì˜ ê²°ê³¼ë¥¼ ë¬¸ìì—´ í˜•íƒœë¡œ ë°˜í™˜ => Dict íƒ€ì…ìœ¼ë¡œ ë³€í™˜ í•„ìš” 
                
                val = validation_res.get("valid")
                msg = validation_res.get("message")
                sql = validation_res.get("sql")
                
                # SQLì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš° ì¡°ê¸° ë°˜í™˜
                # TODO ë‹¤ì‹œ generate_sql ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ë„ë¡ ìˆ˜ì •(íšŸìˆ˜ ì œí•œ)
                if val == False:
                    return f"SQLì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {msg}".strip()
                    
                args["exec_sql"] = str(sql)
                # print('[DEBUG] execute_sql args: ', args)
            
            print(f"\nâ–¶ï¸ ì‹¤í–‰ ì¤‘: {fn_name}({args})")
            
            # ì‹¤í–‰í•  ë„êµ¬(tool_to_call) ì¶”ì¶œ 
            global all_tools
            tool_to_call = None
            for tool in all_tools:
                if tool.name == fn_name:
                    tool_to_call = tool
                    break
            
            # ë„êµ¬(tool_to_call) ì‹¤í–‰
            if tool_to_call:
                try:
                    tool_result = await tool_to_call.ainvoke(args)
                    memory[fn_name] = tool_result
                    print(f"ğŸ†— {fn_name} ê²°ê³¼: {memory[fn_name]}")
                except Exception as e:
                    tool_result = f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
                    memory[fn_name] = tool_result
                    print(f"âŒ {fn_name} ì‹¤í–‰ ì‹¤íŒ¨: {tool_result}")
            else:
                tool_result = f"ë„êµ¬ '{fn_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                memory[fn_name] = tool_result
                print(f"âŒ {tool_result}")

            # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì €ì¥ 
            executed_results.append({
                "function": fn_name,
                "arguments": args,
                "result": str(memory[fn_name])
            })


        # [2ì°¨ ìš”ì²­] ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
        system_prompt_2 = (
            "ë‹¤ìŒì€ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ë¥¼ ì¢…í•©í•´ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•˜ì„¸ìš”:\n\n"
            + json.dumps(executed_results, indent=2, ensure_ascii=False)
        )
        final_payload = {
            "model": OLLAMA_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt_2},
                {"role": "user", "content": question},
            ],
            "stream": False
        }
        async with session.post(f"{OLLAMA_URL}/v1/chat/completions", json=final_payload) as resp2:
            final_data = await resp2.json()
            return final_data["choices"][0]["message"]["content"]


async def main(): 
    # MultiServerMCPClient) ëª¨ë“  ì„œë²„ì™€ ì„¸ì…˜ ê¸°ë°˜ ì—°ê²° ì‹œë„
    try:
        # ëª¨ë“  ì„œë²„ ì„¸ì…˜ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ìˆ˜í–‰)
        await initialize_mcp_sessions()
        
        global all_tools # -> ëª¨ë“  ì„œë²„ì˜ ë„êµ¬ë“¤ì´ ë‹´ê²¨ìˆìŒ 
        
        # ì—°ê²°ëœ ì„œë²„ ë° ì´ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì •ë³´ ì¶œë ¥
        print(f"ğŸ”Œ ì—°ê²°ëœ ì„œë²„: {list(MCP_SERVERS_CONFIG.keys())}")
        if all_tools:
            print(f"âš’ï¸  ì´ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬({len(all_tools)}ê°œ):")
            for tool in all_tools:
                print(f"  - {tool.name}: {getattr(tool, 'description', 'No description')}")
        
        # Ollama LLMì—ê²Œ ì „ë‹¬í•˜ê¸° ìœ„í•œ ë„êµ¬ Description ìƒì„± 
        tool_description = [to_ollama_tool_description(tool) for tool in all_tools]
        
        while True:
            question = input("\nğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (exit ì…ë ¥ ì‹œ ì¢…ë£Œ): ")
            if question.strip().lower() == "exit":
                break
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            await save_message_to_memory("user", question)

            answer = await query_llm(question, tool_description)
            print('\nğŸ¤– LLM ë‹µë³€:\n', answer)
            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥
            await save_message_to_memory("assistant", answer)

    # MultiServerMCPClient) ì„œë²„ì™€ ì„¸ì…˜ ê¸°ë°˜ ì—°ê²° ì‹¤íŒ¨ í–ˆì„ ê²½ìš°
    except Exception as e:
        print(f"âŒ MCP í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹¤íŒ¨: {str(e)}")

    # MultiServerMCPClient) ì—°ê²°ëœ ëª¨ë“  ì„¸ì…˜ ì •ë¦¬
    finally:
        await cleanup_mcp_sessions()


if __name__ == "__main__":
    asyncio.run(main())