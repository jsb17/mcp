import os
import json
import asyncio
from typing import Any, Dict, List

import aiohttp                 # ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸ (Ollama API í˜¸ì¶œìš©)
from dotenv import load_dotenv 
from fastmcp import Client as MCPClient  

load_dotenv()

# Ollama ì„œë²„ ê´€ë ¨ ì„¤ì •
OLLAMA_URL = os.getenv("OLLAMA_URL") 
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL")


# MCP í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ìƒì„±
mcp_client = MCPClient("mcp_server/server_oracle-db.py")


# Utility
def to_ollama_function_description(tool) -> str:
    """
    MCP tools ê´€ë ¨ ì„¤ëª…ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜(promptì— ì‚½ì… ëª©ì )
    """
    raw_schema = (
        getattr(tool, "inputSchema", None)
        or getattr(tool, "input_schema", None)
        or getattr(tool, "parameters", None)
    )

    props = {}
    required = []

    if isinstance(raw_schema, dict):
        props = raw_schema.get("properties", {})
        required = raw_schema.get("required", [])
    elif hasattr(raw_schema, "model_json_schema"):
        schema = raw_schema.model_json_schema()
        props = schema.get("properties", {})
        required = schema.get("required", [])
    elif isinstance(raw_schema, list):  # list of dicts
        props = {p["name"]: {"type": p["type"], "description": p.get("description", "")} for p in raw_schema}
        required = [p["name"] for p in raw_schema if p.get("required", True)]

    lines = [f"í•¨ìˆ˜ ì´ë¦„: {tool.name}"]
    lines.append(f"ì„¤ëª…: {getattr(tool, 'description', '')}")

    if props:
        lines.append("ì¸ì ì„¤ëª…:")
        for name, info in props.items():
            type_ = info.get("type", "unknown")
            desc = info.get("description", "")
            req = " (í•„ìˆ˜)" if name in required else " (ì„ íƒ)"
            lines.append(f"- {name} ({type_}){req}: {desc}")

    return "\n".join(lines)

def strip_code_block(text: str) -> str:
    """
    ```json ... ``` ë˜ëŠ” ``` ... ``` ê°ì‹¼ ë¶€ë¶„ ì œê±°
    """
    if text.strip().startswith("```"):
        return "\n".join(line for line in text.strip().splitlines() if not line.strip().startswith("```"))
    return text

async def query_llm(question: str, tool_descriptions: List[str]) -> str:
    """
    LLM í˜¸ì¶œì„ í†µí•´ ì‚¬ìš©í•  ì ì ˆí•œ toolë“¤ì„ ì„ íƒí•˜ê³ , ì´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
    """
    async with aiohttp.ClientSession() as session:
        # [1ì°¨ ìš”ì²­] MCP ë„êµ¬ë“¤ ì¤‘ ì ì ˆí•œ ê²ƒì„ ì„ íƒí•˜ì—¬ json í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        system_prompt = (
            "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ MCP ë„êµ¬ë¥¼ í™œìš©í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n"
            "ì•„ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì…ë‹ˆë‹¤:\n\n"
            + "\n\n".join(tool_descriptions) +
            "\n\ní•„ìš”í•œ ë„êµ¬ë§Œ ì„ íƒí•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:\n"
            '''[
  {"function_name": "get_schema_info", "arguments": {}},
  {"function_name": "generate_sql", "arguments": {"natural_query": "...", "schema_info": "..." }},
  {"function_name": "validate_sql", "arguments": {"sql": "..." }},
  {"function_name": "execute_sql", "arguments": {"exec_sql": "..." }}
]'''
            "\n\në§Œì¼, ì„ íƒëœ ë„êµ¬ê°€ ì—†ë‹¤ë©´ ì‚¬ì „ì— í•™ìŠµí•œ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•˜ì„¸ìš”:\n"
        )
        payload = {
            "model": OLLAMA_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            "stream": False
        }
        async with session.post(f"{OLLAMA_URL}/v1/chat/completions", json=payload) as resp:
            data = await resp.json()
            first_reply = data["choices"][0]["message"]["content"]
            cleaned_first_reply = strip_code_block(first_reply)
            
        # ë„êµ¬ í˜¸ì¶œ íŒŒì•½
        try: 
            print('\nğŸ“¦ LLM ë„êµ¬ í˜¸ì¶œ ê³„íš:\n', cleaned_first_reply)
            parsed_calls = json.loads(cleaned_first_reply)
        except Exception as e:
            print("\nâŒ ì„ íƒëœ ë„êµ¬ ì—†ìŒ â†’ ë‹µë³€ ê·¸ëŒ€ë¡œ ì¶œë ¥\n")
            return cleaned_first_reply.strip()

        # ì„ íƒëœ ë„êµ¬ íŒŒì‹± í›„ ìˆœì°¨ ì‹¤í–‰
        executed_results = []
        memory = {}  # ì´ì „ ê²°ê³¼ ì €ì¥ìš©
        for call in parsed_calls:
            fn_name = call.get("function_name") or parsed_calls.get("function") or parsed_calls.get("name")
            args = call.get("arguments", {})

            if not fn_name:
                continue

            if fn_name == "generate_sql":
                args["natural_query"] =  question
                args["schema_info"] = memory.get("get_schema_info", {})
                # print('[DEBUG] generate_sql args: ', args)

            if fn_name == "validate_sql":
                args["sql"] = memory.get("generate_sql", "")
                # print('[DEBUG] validate_sql args: ', args)

            if fn_name == "execute_sql":
                validation_res = memory.get("validate_sql", "")  # {"valid": True, "message": "SQL ìœ íš¨í•¨", "sql": sql}
                
                val = validation_res.get("valid")
                msg = validation_res.get("message")
                sql =  validation_res.get("sql")
                
                # TODO ë‹¤ì‹œ generate_sql ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ë„ë¡ ìˆ˜ì •(íšŸìˆ˜ ì œí•œ)
                if val == False:
                    return f"SQLì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {msg}".strip()
                    
                args["exec_sql"] = str(sql)
                # print('[DEBUG] execute_sql args: ', args)
            
            print(f"ğŸ”§ ì‹¤í–‰ ì¤‘: {fn_name}({args})")
            call_tool_result = await mcp_client.call_tool(fn_name, args) # ë°˜í™˜ê°’: CallToolResult()
            # CallToolResult()ì—ì„œ dataë§Œ ì¶”ì¶œ 
            if hasattr(call_tool_result, 'data'):
                tool_result = call_tool_result.data
            memory[fn_name] = tool_result
            print(f"âœ… {fn_name} ê²°ê³¼: {memory[fn_name]}")

            # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ 
            executed_results.append({
                "function": fn_name,
                "arguments": args,
                "result": str(memory[fn_name])
            })


        # [2ì°¨ ìš”ì²­] ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
        system_prompt_2 = (
            "ë‹¤ìŒì€ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ë¥¼ ì¢…í•©í•´ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•˜ì„¸ìš”:\n\n"
            + json.dumps(executed_results, indent=2, ensure_ascii=False)
        )
        final_payload = {
            "model": OLLAMA_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt_2},
                {"role": "user", "content": question},
            ],
            "stream": False
        }
        async with session.post(f"{OLLAMA_URL}/v1/chat/completions", json=final_payload) as resp2:
            final_data = await resp2.json()
            return final_data["choices"][0]["message"]["content"]


async def main(): 
    # MCP í´ë¼ì´ì–¸íŠ¸: ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ MCP ì„œë²„ì— ì—°ê²°, ì‚¬ìš© ëë‚  ì‹œ ìë™ ì—°ê²° í•´ì œ 
    async with mcp_client:
        print(f"MCP connected â†’ {mcp_client.is_connected()}")

        tools = await mcp_client.list_tools() # MCP ì„œë²„ì— ì—°ê²° -> ì„œë²„ì˜ ë„êµ¬ ëª©ë¡ ë¡œë“œ -> LLMì´ ì´í•´ ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ í•„ìš”! 
        tool_description = [to_ollama_function_description(tool) for tool in tools]
        
        while True:
            question = input("\nğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (exit ì…ë ¥ ì‹œ ì¢…ë£Œ): ")
            if question.strip().lower() == "exit":
                break

            answer = await query_llm(question, tool_description)
            print('\nğŸ¤– LLM ë‹µë³€:\n', answer)
    
    print(f"MCP connected â†’ {mcp_client.is_connected()}")


if __name__ == "__main__":
    asyncio.run(main())