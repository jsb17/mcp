"""
generate_answer ë…¸ë“œ - MCP ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ê¸°ë°˜ ìµœì¢… ë‹µë³€ ìƒì„± ë…¸ë“œ
"""

import os
import sys
import json
import aiohttp
import pandas as pd

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from state import AgentState
from utils.utils import get_generate_answer_node_prompt, strip_think_block


async def generate_answer_node(state: AgentState, config) -> AgentState:
        """ìµœì¢… ë‹µë³€ ìƒì„± ë…¸ë“œ"""
        print(f"\nğŸ¤– [generate_answer_node] ì‹œì‘")
        question = state["question"]
        executed_results = state.get("executed_results", [])
        
        if not executed_results:
            print(f"ğŸ¤– [generate_answer_node] ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ì—†ìŒ")
            state["final_answer"] = "ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return state
       
        # "execute_sql"ì´ í¬í•¨ëœ ì•„ì´í…œ ì°¾ê¸°
        execute_sql_item = next(
            (item for item in executed_results if item.get("function") == "execute_sql"),
            None
        )

        if execute_sql_item:
            # execute_sql ì²˜ë¦¬
            executed_results_json = json.loads(execute_sql_item["result"])
            state["dataframe"] = pd.DataFrame(executed_results_json)
            system_prompt = (
                "ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ Text to SQL ì‹¤í–‰ ê²°ê³¼ì…ë‹ˆë‹¤."
                "ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°„ë‹¨í•œ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì œê³µí•˜ì„¸ìš”\n\n"
                + json.dumps(executed_results_json, indent=2, ensure_ascii=False)
            )
        else:
            # execute_sqlì´ ì „í˜€ ì—†ì„ ë•Œ
            system_prompt = get_generate_answer_node_prompt(question, executed_results)
        
        print(f'ğŸ¤– [generate_answer_node] ìµœì¢… ë‹µë³€ SYS prompt: {system_prompt}')

        # ìµœì¢… ë‹µë³€ ìƒì„±
        async with aiohttp.ClientSession() as session:
            payload = {
                "model": config.ollama_model,
                "messages": [
                    {"role": "system", "content": system_prompt}
                ],
                "options": {
                    "temperature": 0,     # ë¬´ì‘ìœ„ì„± ì œê±°
                    "top_p": 1,           # í™•ë¥  ì»·ì˜¤í”„ ì œê±°
                    "repeat_penalty": 1   # ë°˜ë³µ ì–µì œ ì˜í–¥ ìµœì†Œí™”
                },
                "stream": False
            }
            async with session.post(f"{config.ollama_url}/v1/chat/completions", json=payload) as resp:
                data = await resp.json()
                final_answer = data["choices"][0]["message"]["content"]
                # print('ğŸ¤– [generate_answer_node] ìµœì¢… LLM ë‹µë³€:\n', final_answer)
                final_answer = strip_think_block(final_answer) # qwen ëª¨ë¸ íŠ¹í™”
        
        state["final_answer"] = final_answer
        print(f"ğŸ¤– [generate_answer_node] ë‹µë³€ ìƒì„± ì™„ë£Œ")
        return state