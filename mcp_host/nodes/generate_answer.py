"""
generate_answer ë…¸ë“œ - MCP ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ê¸°ë°˜ ìµœì¢… ë‹µë³€ ìƒì„± ë…¸ë“œ
"""

import os
import sys
import json
import aiohttp

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.join(os.path.dirname(__file__), '..')))
from state import AgentState

BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(BASE_DIR)
from utils.utils import get_generate_answer_node_prompt


async def generate_answer_node(state: AgentState, config) -> AgentState:
        """ìµœì¢… ë‹µë³€ ìƒì„± ë…¸ë“œ"""
        print(f"ğŸ¤– [generate_answer_node] ì‹œì‘")
        question = state["question"]
        executed_results = state.get("executed_results", [])
        # state["dataframe"] = pd.DataFrame(executed_results)
        
        if not executed_results:
            print(f"ğŸ¤– [generate_answer_node] ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ì—†ìŒ")
            state["final_answer"] = "ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return state
        
        # ìµœì¢… ë‹µë³€ ìƒì„±
        system_prompt = get_generate_answer_node_prompt(executed_results)
        
        
        async with aiohttp.ClientSession() as session:
            payload = {
                "model": config.ollama_model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ],
                "stream": False
            }
            async with session.post(f"{config.ollama_url}/v1/chat/completions", json=payload) as resp:
                data = await resp.json()
                final_answer = data["choices"][0]["message"]["content"]
                print('\nğŸ¤– LLM ë‹µë³€:\n', final_answer)
        
        state["final_answer"] = final_answer
        print(f"ğŸ¤– [generate_answer_node] ì™„ë£Œ")
        return state